{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align:right\";> *Elements Logiciels Pour Le Traitement De Données Massives - ENSAE ParisTech - Janvier 2018*</p>  <p style=\"text-align:right\";> Gilles Cornec - Samuel Ritchie </p>\n",
    "\n",
    "# <p style=\"text-align:center\";><span style=\"color: #fb4141\">Sentiment analysis - Classifying tweets using PySpark MapReduce Approach</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Abstract:**_ _Ce notebook présente une approache PySpark afin de répondre à un problème de sentiment analysis : la classification de tweets. Les données sont issues de http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/, qui regroupe un DataSet d'entraînement composées de 1,578,627 tweets. Chaque ligne est labélisée par un 1 pour un sentiment 'positif' et par un 0 pour un sentiment 'négatif'.\n",
    "Dans ce notebook, nous présentons dans un premier temps la création de la base par approche MapReduce et dans un second temps divers algorithmes de classification binaire (Régression logistique, Naive Bayes) parallélisés par nos soins, que nous comparons aux algorithmes de la librairie MLLib de Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"my_id_menu_nb\">run previous cell, wait for 2 seconds</div>\n",
       "<script>\n",
       "function repeat_indent_string(n){\n",
       "    var a = \"\" ;\n",
       "    for ( ; n > 0 ; --n)\n",
       "        a += \"    \";\n",
       "    return a;\n",
       "}\n",
       "var update_menu_string = function(begin, lfirst, llast, sformat, send, keep_item, begin_format, end_format) {\n",
       "    var anchors = document.getElementsByClassName(\"section\");\n",
       "    if (anchors.length == 0) {\n",
       "        anchors = document.getElementsByClassName(\"text_cell_render rendered_html\");\n",
       "    }\n",
       "    var i,t;\n",
       "    var text_menu = begin;\n",
       "    var text_memo = \"<pre>\\nlength:\" + anchors.length + \"\\n\";\n",
       "    var ind = \"\";\n",
       "    var memo_level = 1;\n",
       "    var href;\n",
       "    var tags = [];\n",
       "    var main_item = 0;\n",
       "    var format_open = 0;\n",
       "    for (i = 0; i <= llast; i++)\n",
       "        tags.push(\"h\" + i);\n",
       "\n",
       "    for (i = 0; i < anchors.length; i++) {\n",
       "        text_memo += \"**\" + anchors[i].id + \"--\\n\";\n",
       "\n",
       "        var child = null;\n",
       "        for(t = 0; t < tags.length; t++) {\n",
       "            var r = anchors[i].getElementsByTagName(tags[t]);\n",
       "            if (r.length > 0) {\n",
       "child = r[0];\n",
       "break;\n",
       "            }\n",
       "        }\n",
       "        if (child == null) {\n",
       "            text_memo += \"null\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        if (anchors[i].hasAttribute(\"id\")) {\n",
       "            // when converted in RST\n",
       "            href = anchors[i].id;\n",
       "            text_memo += \"#1-\" + href;\n",
       "            // passer à child suivant (le chercher)\n",
       "        }\n",
       "        else if (child.hasAttribute(\"id\")) {\n",
       "            // in a notebook\n",
       "            href = child.id;\n",
       "            text_memo += \"#2-\" + href;\n",
       "        }\n",
       "        else {\n",
       "            text_memo += \"#3-\" + \"*\" + \"\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        var title = child.textContent;\n",
       "        var level = parseInt(child.tagName.substring(1,2));\n",
       "\n",
       "        text_memo += \"--\" + level + \"?\" + lfirst + \"--\" + title + \"\\n\";\n",
       "\n",
       "        if ((level < lfirst) || (level > llast)) {\n",
       "            continue ;\n",
       "        }\n",
       "        if (title.endsWith('¶')) {\n",
       "            title = title.substring(0,title.length-1).replace(\"<\", \"&lt;\")\n",
       "         .replace(\">\", \"&gt;\").replace(\"&\", \"&amp;\");\n",
       "        }\n",
       "        if (title.length == 0) {\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        while (level < memo_level) {\n",
       "            text_menu += end_format + \"</ul>\\n\";\n",
       "            format_open -= 1;\n",
       "            memo_level -= 1;\n",
       "        }\n",
       "        if (level == lfirst) {\n",
       "            main_item += 1;\n",
       "        }\n",
       "        if (keep_item != -1 && main_item != keep_item + 1) {\n",
       "            // alert(main_item + \" - \" + level + \" - \" + keep_item);\n",
       "            continue;\n",
       "        }\n",
       "        while (level > memo_level) {\n",
       "            text_menu += \"<ul>\\n\";\n",
       "            memo_level += 1;\n",
       "        }\n",
       "        text_menu += repeat_indent_string(level-2);\n",
       "        text_menu += begin_format + sformat.replace(\"__HREF__\", href).replace(\"__TITLE__\", title);\n",
       "        format_open += 1;\n",
       "    }\n",
       "    while (1 < memo_level) {\n",
       "        text_menu += end_format + \"</ul>\\n\";\n",
       "        memo_level -= 1;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    text_menu += send;\n",
       "    //text_menu += \"\\n\" + text_memo;\n",
       "\n",
       "    while (format_open > 0) {\n",
       "        text_menu += end_format;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    return text_menu;\n",
       "};\n",
       "var update_menu = function() {\n",
       "    var sbegin = \"\";\n",
       "    var sformat = '<a href=\"#__HREF__\">__TITLE__</a>';\n",
       "    var send = \"\";\n",
       "    var begin_format = '<li>';\n",
       "    var end_format = '</li>';\n",
       "    var keep_item = -1;\n",
       "    var text_menu = update_menu_string(sbegin, 2, 4, sformat, send, keep_item,\n",
       "       begin_format, end_format);\n",
       "    var menu = document.getElementById(\"my_id_menu_nb\");\n",
       "    menu.innerHTML=text_menu;\n",
       "};\n",
       "window.setTimeout(update_menu,2000);\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%pylab inline\n",
    "from jyquickhelper import add_notebook_menu\n",
    "add_notebook_menu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Création d'un environnement PySpark - Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.41.183:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ELTDM_CORNEC_RITCHIE</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10e03e128>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ELTDM_CORNEC_RITCHIE\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "  \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réalisé en 0.608 secondes\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "bdd = sc.textFile(\"Sentiment_Analysis_Dataset.csv\").map(lambda line: line.split(\",\"))\n",
    "t1 = time() - t0\n",
    "print(\"Réalisé en {} secondes\".format(round(t1,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La base de donées contient 1578628 tweets.\n",
      "Réalisé en 5.248 secondes\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "print('La base de donées contient ' + str(bdd.count()) + ' tweets.')\n",
    "t1 = time() - t0\n",
    "print(\"Réalisé en {} secondes\".format(round(t1,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ItemID', 'Sentiment', 'SentimentSource', 'SentimentText'], ['1', '0', 'Sentiment140', '                     is so sad for my APL friend.............']]\n"
     ]
    }
   ],
   "source": [
    "#Afficher une ligne de la base\n",
    "print(bdd.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Travail préliminaire sur la base de données par MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, on sépare le texte du tweet simplement en séparant grâce à la fonction strip et on transforme tous les mots en minuscule. Ceci est effectué en mappant sur toutes les lignes la fonction split en environnement Spark. On obtient de cette manière une liste de mots pour chaque tweet. On récupère également son label 0-1 qui nous indique s'il s'agit d'un tweet positif ou négatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(tweet):\n",
    "    tweet_list = []\n",
    "    word_list = re.split('\\W+',tweet.strip())\n",
    "    for w in word_list: \n",
    "        if w != '':\n",
    "            tweet_list.append(w.lower()) # append words in lowercase to their corresponding file\n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdd_test = bdd.map(lambda x: (x[0],x[1], split(x[3])))\n",
    "header = bdd_test.first()\n",
    "bdd_test = bdd_test.filter(lambda line: line != header)\n",
    "bdd_test = sc.parallelize(bdd_test.take(10000)) #On teste sur un sous-échantillon des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On représente les données sous la forme d'un tableau (environnement DataFrame de PySpark, différent de l'environnement RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "| ID|label|               tweet|\n",
      "+---+-----+--------------------+\n",
      "|  1|    0|[is, so, sad, for...|\n",
      "|  2|    0|[i, missed, the, ...|\n",
      "|  3|    1|[omg, its, alread...|\n",
      "|  4|    0|[omgaga, im, sooo...|\n",
      "|  5|    0|[i, think, mi, bf...|\n",
      "|  6|    0|[or, i, just, wor...|\n",
      "|  7|    1|[juuuuuuuuuuuuuuu...|\n",
      "|  8|    0|[sunny, again, wo...|\n",
      "|  9|    1|[handed, in, my, ...|\n",
      "| 10|    1|[hmmmm, i, wonder...|\n",
      "| 11|    0|[i, must, think, ...|\n",
      "| 12|    1|[thanks, to, all,...|\n",
      "| 13|    0|[this, weekend, h...|\n",
      "| 14|    0|[jb, isnt, showin...|\n",
      "| 15|    0|[ok, thats, it, y...|\n",
      "| 16|    0|[lt, this, is, th...|\n",
      "| 17|    0|[awhhe, man, i, m...|\n",
      "| 18|    1|[feeling, strange...|\n",
      "| 19|    0|[huge, roll, of, ...|\n",
      "| 20|    0|[i, just, cut, my...|\n",
      "+---+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "rdd_row = bdd_test.map(lambda line: Row(ID = line[0],label = line[1],tweet = line[2]))\n",
    "df = spark.createDataFrame(rdd_row)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sépare les données en train et test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train, df_test = df.randomSplit([0.6, 0.4], seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Hashing du texte à la main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, on retravaille la base de données pour pouvoir l'exploiter dans un second temps dans des algorithmes de classifications. L'idée est de créer un certain nombre de colonnes traduisant l'importance des mots au sein de chaque tweet / dans l'ensemble des tweets. Dans la mesure où le projet se concentre sur l'utilisation et la découverte de PySpark, nous n'avons pas poussé très loin ce pré-traitement. Dans un premier temps, on définit manuellement les colonnes en passant par des fonctions de hashage, puis dans un second temps nous utiliserons l'approche TF-IDF en se basant sur la librairie sparkMLLib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit tout d'abord les occurences des mots pour chaque tweet en appliquant un map sur l'ensemble des lignes des colonnes, et ce pour train et test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0',\n",
       "  [('is', 0.14285714285714285),\n",
       "   ('so', 0.14285714285714285),\n",
       "   ('sad', 0.14285714285714285),\n",
       "   ('for', 0.14285714285714285),\n",
       "   ('my', 0.14285714285714285),\n",
       "   ('apl', 0.14285714285714285),\n",
       "   ('friend', 0.14285714285714285)]),\n",
       " ('1',\n",
       "  [('hmmmm', 0.14285714285714285),\n",
       "   ('i', 0.14285714285714285),\n",
       "   ('wonder', 0.14285714285714285),\n",
       "   ('how', 0.14285714285714285),\n",
       "   ('she', 0.14285714285714285),\n",
       "   ('my', 0.14285714285714285),\n",
       "   ('number', 0.14285714285714285)])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "train = df_train.rdd.map(lambda x: (x[1],list(Counter(x[2]).items())))\n",
    "test = df_test.rdd.map(lambda x: (x[1],list(Counter(x[2]).items())))\n",
    "#train.take(2)\n",
    "train = train.map(lambda vec:(vec[0],[(w,c/sum([c for (w, c) in vec[1]]))for (w,c) in vec[1]])) # normalise\n",
    "test = test.map(lambda vec:(vec[0],[(w,c/sum([c for (w, c) in vec[1]]))for (w,c) in vec[1]])) # normalise\n",
    "train.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On passe ensuite ces listes de mots par une fonction de hashage, en fixant nous même le nombre de colonnes utilisées. Plus ce nombre est élevé, plus la précision de la régression menée en aval sera grande mais plus le temps d'exécution de l'alogorithme sera long "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hashing(liste_mots, N): \n",
    "    v = [0] * N  \n",
    "    for mot_count in liste_mots: \n",
    "        mot,count = mot_count\n",
    "        h = hash(mot)\n",
    "        v[h % N] = v[h % N] + count \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0', [0, 0, 0, 0.14285714285714285, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.14285714285714285, 0, 0, 0, 0.14285714285714285, 0, 0, 0, 0, 0, 0, 0.14285714285714285, 0, 0, 0, 0, 0, 0, 0, 0, 0.14285714285714285, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2857142857142857, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), ('1', [0, 0.14285714285714285, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.14285714285714285, 0, 0, 0, 0, 0, 0, 0, 0.14285714285714285, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.14285714285714285, 0.14285714285714285, 0, 0, 0, 0.14285714285714285, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.14285714285714285, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), ('0', [0.076923076923076927, 0, 0, 0, 0, 0, 0.038461538461538464, 0, 0.038461538461538464, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0, 0, 0, 0, 0, 0, 0.076923076923076927, 0, 0, 0.038461538461538464, 0, 0, 0, 0.038461538461538464, 0, 0, 0, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0, 0.038461538461538464, 0, 0.11538461538461539, 0, 0, 0.038461538461538464, 0, 0, 0, 0, 0, 0, 0, 0.038461538461538464, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.076923076923076927, 0, 0, 0, 0, 0, 0, 0, 0, 0.038461538461538464, 0, 0.038461538461538464, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.038461538461538464, 0.038461538461538464, 0, 0, 0]), ('1', [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.33333333333333331, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.33333333333333331, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.33333333333333331, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "def hashing_liste(df, N): # function (g) applies the hashing vectoriser\n",
    "    liste = df.map(lambda x: (x[0],hashing(x[1],N))) \n",
    "    return liste\n",
    "\n",
    "N=100\n",
    "train = hashing_liste(train,N)\n",
    "test = hashing_liste(test,N)\n",
    "print(train.take(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. TF-IDF approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF est un acronyme pour Term Frequency - Inverse Document Frequency. Son but est de juger de l'importance d'un mot dans un corpus. Cette valeur correspond à un équilibre entre l'occurence du mot dans le tweet et son occurence dans le corpus. Ainsi l'occurence d'un mot dans un tweet est pondéré par sa fréquence dans un corpus donné. \n",
    "En terme mathématique, le Term Frequency TF() d'un mot m dans un tweet t correspond au nombre de fois ou le mot apparait dans le document.\n",
    "$$ TF(m; t) = f_{m;t} $$\n",
    "L'Inverse Document Frequency est une mesure de l'information que la présence du mot donne, c'est à dire à quel point il est commun ou rare dans le corpus entier. Il est donné par la formule suivante:\n",
    "\n",
    "$$IDF(m;C) = log(\\frac{card(C)}{\\{t \\in C, m \\in t \\}})$$\n",
    "\n",
    "Enfin, le TF-IDF est calculé comme la multiplication de ces deux derniers termes.\n",
    "Ainsi, une valeur importante montre soit que le mot apparait dans beaucoup de tweets soit qu'il apparait peu de fois sur l'ensemble des tweets.\n",
    "\n",
    "On utilise ici les fonctions pré-implémentées par la libraire pysparkML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ID  |label|tweet                                                                                                                                                       |tf                                                                                                                                                                                                                                                           |\n",
      "+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1   |0    |[is, so, sad, for, my, apl, friend]                                                                                                                         |(262144,[15889,16332,37852,87401,158870,188424,258668],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                        |\n",
      "|10  |1    |[hmmmm, i, wonder, how, she, my, number]                                                                                                                    |(262144,[24417,37852,58370,114980,172634,229103,246349],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                       |\n",
      "|100 |0    |[no, movie, times, for, sunday, rats, will, have, to, plan, tomorrow, i, guess, this, means, i, have, to, work, on, those, two, presentations, i, am, doing]|(262144,[5173,13396,15664,16332,24417,29129,34343,58513,81948,89356,99736,100258,108541,120348,141522,151146,155321,156250,166629,205044,232427,253475],[1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0])           |\n",
      "|1000|1    |[fucking, rur, mom]                                                                                                                                         |(262144,[53284,202660,237168],[1.0,1.0,1.0])                                                                                                                                                                                                                 |\n",
      "|1003|0    |[gcse, s, clearly, suck]                                                                                                                                    |(262144,[76549,94533,138937,157209],[1.0,1.0,1.0,1.0])                                                                                                                                                                                                       |\n",
      "|1007|1    |[getting, a, mani, pedi, with, the, husband]                                                                                                                |(262144,[18410,34140,103838,126466,166366,224432,227410],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                      |\n",
      "|1008|1    |[getting, a, webcam, today]                                                                                                                                 |(262144,[34140,143894,227410,228469],[1.0,1.0,1.0,1.0])                                                                                                                                                                                                      |\n",
      "|1009|0    |[getting, ready, to, leave, for, spring, city]                                                                                                              |(262144,[16332,34140,83756,86290,92900,205044,229543],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                         |\n",
      "|101 |0    |[no, pavel, tonight, lt, tigersfan, gt]                                                                                                                     |(262144,[53013,89710,93917,95978,156250,228453],[1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                   |\n",
      "|1010|1    |[getting, ready, to, watch, the, sprint, cup, boys, try, to, tame, the, concrete, monster, mile, hopefully, tires, won, t, be, too, much, of, an, issue]    |(262144,[9639,13356,20998,21238,34140,46044,73062,76764,79876,92900,95457,103838,141407,147337,159636,167152,173716,193352,194536,205044,209203,222888,254274],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0])|\n",
      "|1014|0    |[go, away, migraine, you, are, not, welcome]                                                                                                                |(262144,[9129,81213,139098,147379,167122,172477,252801],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                       |\n",
      "|1015|0    |[go, away, rain]                                                                                                                                            |(262144,[9129,172477,193254],[1.0,1.0,1.0])                                                                                                                                                                                                                  |\n",
      "|1017|1    |[go, wings, ftw]                                                                                                                                            |(262144,[166416,172477,246525],[1.0,1.0,1.0])                                                                                                                                                                                                                |\n",
      "|1018|1    |[god, i, m, up, early, hayley, still, asleep, but, today, is, party, day, so, i, m, getting, stuff, ready, x]                                               |(262144,[13957,15889,18910,24417,33209,33524,34140,35028,36200,57304,92900,133942,138018,143894,152575,179344,188424,189683],[1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                      |\n",
      "|102 |0    |[not, a, cool, night]                                                                                                                                       |(262144,[66092,139098,223619,227410],[1.0,1.0,1.0,1.0])                                                                                                                                                                                                      |\n",
      "|1021|0    |[going, to, bed]                                                                                                                                            |(262144,[87730,205044,232685],[1.0,1.0,1.0])                                                                                                                                                                                                                 |\n",
      "|1022|1    |[going, to, bed]                                                                                                                                            |(262144,[87730,205044,232685],[1.0,1.0,1.0])                                                                                                                                                                                                                 |\n",
      "|1023|1    |[going, to, bed]                                                                                                                                            |(262144,[87730,205044,232685],[1.0,1.0,1.0])                                                                                                                                                                                                                 |\n",
      "|1024|0    |[going, to, get, piï, ½a, colada, mix, to, wallow, in, my, sorrows]                                                                                         |(262144,[15786,27884,37097,37852,84860,98568,99895,203325,205044,222453,232685],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0])                                                                                                                               |\n",
      "|1025|0    |[going, to, have, such, a, bad, day]                                                                                                                        |(262144,[13957,96638,180008,205044,227410,232685,253475],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                      |\n",
      "+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF as MLHashingTF\n",
    "from pyspark.ml.feature import IDF as MLIDF\n",
    "htf = MLHashingTF(inputCol=\"tweet\", outputCol=\"tf\")\n",
    "tf_train = htf.transform(df_train)\n",
    "tf_test = htf.transform(df_test)\n",
    "tf_train.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ID  |label|tweet                                                                                                                                                       |tf                                                                                                                                                                                                                                                           |idf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1   |0    |[is, so, sad, for, my, apl, friend]                                                                                                                         |(262144,[15889,16332,37852,87401,158870,188424,258668],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                        |(262144,[15889,16332,37852,87401,158870,188424,258668],[2.0454397444023464,2.346125901277334,1.8238477837795557,8.003864437432128,5.14166355650266,2.6639253961932723,3.799171818041162])                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|10  |1    |[hmmmm, i, wonder, how, she, my, number]                                                                                                                    |(262144,[24417,37852,58370,114980,172634,229103,246349],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                       |(262144,[24417,37852,58370,114980,172634,229103,246349],[1.1107153108393533,1.8238477837795557,6.499787040655854,8.003864437432128,3.969623799279733,5.478135793123873,3.7770306921639487])                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|100 |0    |[no, movie, times, for, sunday, rats, will, have, to, plan, tomorrow, i, guess, this, means, i, have, to, work, on, those, two, presentations, i, am, doing]|(262144,[5173,13396,15664,16332,24417,29129,34343,58513,81948,89356,99736,100258,108541,120348,141522,151146,155321,156250,166629,205044,232427,253475],[1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0])           |(262144,[5173,13396,15664,16332,24417,29129,34343,58513,81948,89356,99736,100258,108541,120348,141522,151146,155321,156250,166629,205044,232427,253475],[5.231275715192347,4.912821984073813,5.263024413506927,2.346125901277334,3.33214593251806,4.278171010195476,3.7770306921639487,8.003864437432128,5.806639860095909,3.821814294790922,5.605969164633758,2.552825983866428,2.7489766288114277,7.598399329323964,5.478135793123873,5.988961416889864,5.033449971862427,3.3788916241478573,3.8607297110405954,3.0847925221568215,6.617570076312238,5.599715500710666])                        |\n",
      "|1000|1    |[fucking, rur, mom]                                                                                                                                         |(262144,[53284,202660,237168],[1.0,1.0,1.0])                                                                                                                                                                                                                 |(262144,[53284,202660,237168],[8.003864437432128,5.2005040565255936,5.752572638825633])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|1003|0    |[gcse, s, clearly, suck]                                                                                                                                    |(262144,[76549,94533,138937,157209],[1.0,1.0,1.0,1.0])                                                                                                                                                                                                       |(262144,[76549,94533,138937,157209],[6.0579542883768145,2.540032632406518,8.003864437432128,7.598399329323964])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|1007|1    |[getting, a, mani, pedi, with, the, husband]                                                                                                                |(262144,[18410,34140,103838,126466,166366,224432,227410],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                      |(262144,[18410,34140,103838,126466,166366,224432,227410],[6.499787040655854,4.538128534632402,1.5978098744389813,3.0375294022324524,7.087573705557973,7.3107172568721825,1.7972885107072007])                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|1008|1    |[getting, a, webcam, today]                                                                                                                                 |(262144,[34140,143894,227410,228469],[1.0,1.0,1.0,1.0])                                                                                                                                                                                                      |(262144,[34140,143894,227410,228469],[4.538128534632402,3.5553480614894135,1.7972885107072007,7.598399329323964])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|1009|0    |[getting, ready, to, leave, for, spring, city]                                                                                                              |(262144,[16332,34140,83756,86290,92900,205044,229543],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                         |(262144,[16332,34140,83756,86290,92900,205044,229543],[2.346125901277334,4.538128534632402,4.959341999708705,7.598399329323964,5.295814236329918,1.5423962610784108,6.0579542883768145])                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|101 |0    |[no, pavel, tonight, lt, tigersfan, gt]                                                                                                                     |(262144,[53013,89710,93917,95978,156250,228453],[1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                   |(262144,[53013,89710,93917,95978,156250,228453],[8.003864437432128,3.3691354492024925,4.29029237072782,8.003864437432128,3.3788916241478573,4.208375248259934])                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|1010|1    |[getting, ready, to, watch, the, sprint, cup, boys, try, to, tame, the, concrete, monster, mile, hopefully, tires, won, t, be, too, much, of, an, issue]    |(262144,[9639,13356,20998,21238,34140,46044,73062,76764,79876,92900,95457,103838,141407,147337,159636,167152,173716,193352,194536,205044,209203,222888,254274],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0])|(262144,[9639,13356,20998,21238,34140,46044,73062,76764,79876,92900,95457,103838,141407,147337,159636,167152,173716,193352,194536,205044,209203,222888,254274],[2.3478726266122756,8.003864437432128,4.868370221502978,6.75110146893676,4.538128534632402,8.003864437432128,8.003864437432128,4.052620718850701,5.652489180268651,5.295814236329918,2.466530170413592,3.1956197488779625,5.561517402062924,7.598399329323964,4.112044139321502,2.8948932426150105,8.003864437432128,7.087573705557973,4.091841432003982,3.0847925221568215,8.003864437432128,5.863798273935857,4.805191319881447])|\n",
      "|1014|0    |[go, away, migraine, you, are, not, welcome]                                                                                                                |(262144,[9129,81213,139098,147379,167122,172477,252801],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                       |(262144,[9129,81213,139098,147379,167122,172477,252801],[4.671659927256925,5.924422895752293,2.9444389791664403,7.3107172568721825,3.2809112157876537,3.4985145867262477,2.411013523483209])                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|1015|0    |[go, away, rain]                                                                                                                                            |(262144,[9129,172477,193254],[1.0,1.0,1.0])                                                                                                                                                                                                                  |(262144,[9129,172477,193254],[4.671659927256925,3.4985145867262477,5.401174751987744])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|1017|1    |[go, wings, ftw]                                                                                                                                            |(262144,[166416,172477,246525],[1.0,1.0,1.0])                                                                                                                                                                                                                |(262144,[166416,172477,246525],[6.394426524998027,3.4985145867262477,6.905252148764019])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|1018|1    |[god, i, m, up, early, hayley, still, asleep, but, today, is, party, day, so, i, m, getting, stuff, ready, x]                                               |(262144,[13957,15889,18910,24417,33209,33524,34140,35028,36200,57304,92900,133942,138018,143894,152575,179344,188424,189683],[1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                      |(262144,[13957,15889,18910,24417,33209,33524,34140,35028,36200,57304,92900,133942,138018,143894,152575,179344,188424,189683],[3.2898398465319545,2.0454397444023464,5.826372871324672,2.2214306216787065,5.652489180268651,5.561517402062924,4.538128534632402,5.231275715192347,3.943421426885709,5.170651093375912,5.295814236329918,8.003864437432128,5.170651093375912,3.5553480614894135,6.212104968204073,3.151834173512511,2.6639253961932723,3.1715586788602894])                                                                                                                         |\n",
      "|102 |0    |[not, a, cool, night]                                                                                                                                       |(262144,[66092,139098,223619,227410],[1.0,1.0,1.0,1.0])                                                                                                                                                                                                      |(262144,[66092,139098,223619,227410],[3.978512746696979,2.9444389791664403,4.890349128221754,1.7972885107072007])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|1021|0    |[going, to, bed]                                                                                                                                            |(262144,[87730,205044,232685],[1.0,1.0,1.0])                                                                                                                                                                                                                 |(262144,[87730,205044,232685],[4.689678432759602,1.5423962610784108,3.7341669877321664])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|1022|1    |[going, to, bed]                                                                                                                                            |(262144,[87730,205044,232685],[1.0,1.0,1.0])                                                                                                                                                                                                                 |(262144,[87730,205044,232685],[4.689678432759602,1.5423962610784108,3.7341669877321664])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|1023|1    |[going, to, bed]                                                                                                                                            |(262144,[87730,205044,232685],[1.0,1.0,1.0])                                                                                                                                                                                                                 |(262144,[87730,205044,232685],[4.689678432759602,1.5423962610784108,3.7341669877321664])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|1024|0    |[going, to, get, piï, ½a, colada, mix, to, wallow, in, my, sorrows]                                                                                         |(262144,[15786,27884,37097,37852,84860,98568,99895,203325,205044,222453,232685],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0])                                                                                                                               |(262144,[15786,27884,37097,37852,84860,98568,99895,203325,205044,222453,232685],[7.3107172568721825,8.003864437432128,8.003864437432128,1.8238477837795557,7.3107172568721825,6.905252148764019,3.25459390747028,8.003864437432128,3.0847925221568215,2.291783159961232,3.7341669877321664])                                                                                                                                                                                                                                                                                                      |\n",
      "|1025|0    |[going, to, have, such, a, bad, day]                                                                                                                        |(262144,[13957,96638,180008,205044,227410,232685,253475],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                      |(262144,[13957,96638,180008,205044,227410,232685,253475],[3.2898398465319545,4.278171010195476,5.231275715192347,1.5423962610784108,1.7972885107072007,3.7341669877321664,2.799857750355333])                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf = MLIDF(inputCol=\"tf\", outputCol=\"idf\")\n",
    "tfidf_train = idf.fit(tf_train).transform(tf_train)\n",
    "tfidf_test = idf.fit(tf_test).transform(tf_test)\n",
    "tfidf_train.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_train = tfidf_train.rdd.map(lambda x : (x.label,x.tweet,x.tf,x.idf,(None if x.idf is None else x.idf.values.sum())))\n",
    "res_test = tfidf_test.rdd.map(lambda x : (x.label,x.tweet,x.tf,x.idf,(None if x.idf is None else x.idf.values.sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import linalg as ml_linalg\n",
    "from pyspark.mllib.linalg import Vector as MLLibVector, Vectors as MLLibVectors\n",
    "\n",
    "def as_mllib(v):\n",
    "    if isinstance(v, ml_linalg.SparseVector):\n",
    "        return MLLibVectors.sparse(v.size, v.indices, v.values)\n",
    "    elif isinstance(v, ml_linalg.DenseVector):\n",
    "        return MLLibVectors.dense(v.toArray())\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported type: {0}\".format(type(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Création d'objets RDD 'LabeledPoint' pour la classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'environnement PySpark possède un objet 'LabeledPoint' très utile pour mener dans un second temps des algorithmes de classification. On définit ici de nouveaux RDD contenant ces objets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label_point_RDD(df): # function (f) creates labelled points where 1=spam; 0=non-spam and works when the argument inp is either a path (when trg =='path') or an RDD (when trg==''). The latter becomes useful at Task e  \n",
    "    label_point_RDD = df.map(lambda x: LabeledPoint(0 if (x[0]=='0') else 1,x[1])) # assign labelled points\n",
    "    return label_point_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0', [0, 0, 0, 0.14285714285714285, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.14285714285714285, 0, 0, 0, 0.14285714285714285, 0, 0, 0, 0, 0, 0, 0.14285714285714285, 0, 0, 0, 0, 0, 0, 0, 0, 0.14285714285714285, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2857142857142857, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), ('1', [0, 0.14285714285714285, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.14285714285714285, 0, 0, 0, 0, 0, 0, 0, 0.14285714285714285, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.14285714285714285, 0.14285714285714285, 0, 0, 0, 0.14285714285714285, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.14285714285714285, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), ('0', [0.076923076923076927, 0, 0, 0, 0, 0, 0.038461538461538464, 0, 0.038461538461538464, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0, 0, 0, 0, 0, 0, 0.076923076923076927, 0, 0, 0.038461538461538464, 0, 0, 0, 0.038461538461538464, 0, 0, 0, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0, 0.038461538461538464, 0, 0.11538461538461539, 0, 0, 0.038461538461538464, 0, 0, 0, 0, 0, 0, 0, 0.038461538461538464, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.076923076923076927, 0, 0, 0, 0, 0, 0, 0, 0, 0.038461538461538464, 0, 0.038461538461538464, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.038461538461538464, 0.038461538461538464, 0, 0, 0]), ('1', [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.33333333333333331, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.33333333333333331, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.33333333333333331, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "train_label = make_label_point_RDD(train)\n",
    "test_label = make_label_point_RDD(test)\n",
    "print(train.take(4)) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label_point_RDD_2(df): \n",
    "    label_point_RDD = df.map(lambda x: LabeledPoint(0 if (x[0]=='0') else 1,as_mllib(x[3]))) # assign labelled points\n",
    "    return label_point_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, (262144,[15889,16332,37852,87401,158870,188424,258668],[2.0454397444,2.34612590128,1.82384778378,8.00386443743,5.1416635565,2.66392539619,3.79917181804])),\n",
       " LabeledPoint(1.0, (262144,[24417,37852,58370,114980,172634,229103,246349],[1.11071531084,1.82384778378,6.49978704066,8.00386443743,3.96962379928,5.47813579312,3.77703069216]))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_2 = make_label_point_RDD_2(res_train) # uses function (f) to build labelled points when argument inp is a path (i.e. trg=='path')\n",
    "test_label_2 = make_label_point_RDD_2(res_test)\n",
    "train_label_2.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithmes de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Avec la librairie MLLib (basé sur objets RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, NaiveBayes, SVMWithSGD\n",
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_test_model(train,test): # function(g) trains a logistic regression model to detect spam vs. non-spam files\n",
    "    logReg_model = LogisticRegressionWithLBFGS.train(train) # train the algorithm\n",
    "    #logReg_model = NaiveBayes.train(train)\n",
    "    #logReg_model = SVMWithSGD.train(train)\n",
    "    correct_train = train.map(lambda lp: 1 if logReg_model.predict(lp.features) == lp.label else 0).sum() # calculate correctly classified data points\n",
    "    correct_test = test.map(lambda lp: 1 if logReg_model.predict(lp.features) == lp.label else 0).sum() # calculate correctly classified data points\n",
    "    count_train = train.count() # counts the size of training set\n",
    "    count_test = test.count() # counts the size of test set\n",
    "    #print('training Logistic Regression with Limited-memory the Broyden–Fletcher–Goldfarb–Shanno (BFGS)')\n",
    "    print('Training data items: {}, Correct: {}'.format(count_train, correct_train))\n",
    "    print('Test data items: {}, Correct: {}'.format(count_test, correct_test))\n",
    "    print('Train accuracy {:.1%}'.format(correct_train/count_train)) \n",
    "    print('Test accuracy {:.1%}'.format(correct_test/count_test)) \n",
    "    return logReg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data items: 5984, Correct: 3968\n",
      "Test data items: 4016, Correct: 2616\n",
      "Train accuracy 66.3%\n",
      "Test accuracy 65.1%\n"
     ]
    }
   ],
   "source": [
    "logReg_model = train_and_test_model(train_label,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data items: 5984, Correct: 5973\n",
      "Test data items: 4016, Correct: 2821\n",
      "Train accuracy 99.8%\n",
      "Test accuracy 70.2%\n"
     ]
    }
   ],
   "source": [
    "logReg_model = train_and_test_model(train_label_2,test_label_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. En parallélisant manuellement la descente de gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit dans un premier temps les fonctions utiles pour effectuer la descente de gradient, que l'on utilisera lors de l'étape map_reduce dans un second temps. A ce titre, on définit la fonction logistique ainsi que le gradient associé à cette fonction objectif (calculé à la main). On définit également la fonction score qui permet d'indiquer si la classification est réussie ou non (en fixant un seuil à 0.5 puisque la régression logistique prédit une probabilité), et enfin une fonction add utile pour l'étape \"reduce\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(x, w):\n",
    "    return  1.0 / (1.0 + np.exp(-(np.asarray(x).dot(w))))\n",
    "\n",
    "def logistic_gradient(x, y, w): \n",
    "    return (x * ((1.0 / (1.0 + np.exp(-np.asarray(x*w)))) - y))\n",
    "\n",
    "def gradient(matrix, w):\n",
    "    Y = matrix[0]\n",
    "    X = matrix[1:]\n",
    "    return logistic_gradient(X, Y, w)\n",
    "\n",
    "def score(matrix, w):\n",
    "    Y = matrix[0]\n",
    "    X = matrix[1:]\n",
    "    pred = logistic(X, w)\n",
    "    return accuracy_score(np.array([Y > 0.5]) , np.array([pred > 0.5]), normalize=False)\n",
    "\n",
    "def add(x, y):\n",
    "    x += y\n",
    "    return x\n",
    "\n",
    "w = 2 * np.random.ranf(N) - 1\n",
    "wb = sc.broadcast(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On transforme les données de train et de test au bon format pour pouvoir appliquer la descente de gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_main = train.map(lambda x: [int(x[0])] + x[1])\n",
    "test_main = test.map(lambda x: [int(x[0])] + x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fixe ici un nombre d'itérations de la descente de gradient à la main. Il aurait également été envisageable de définir un seuil de variations entre deux valeurs consécutives pour arrêter l'algorithme.\n",
    "On retrouve ensuite le pattern typique MapReduce : on applique le gradient à chaque ligne qui contient le label et les features, puis on somme ce gradient, et ce pour le nombre d'itérations choisi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itération 0 : Train Score 0.59  ; Test Score 0.59\n",
      "Itération 1 : Train Score 0.522  ; Test Score 0.528\n",
      "Itération 2 : Train Score 0.599  ; Test Score 0.596\n",
      "Itération 3 : Train Score 0.594  ; Test Score 0.593\n",
      "Itération 4 : Train Score 0.605  ; Test Score 0.6\n",
      "Itération 5 : Train Score 0.595  ; Test Score 0.591\n",
      "Itération 6 : Train Score 0.59  ; Test Score 0.58\n",
      "Itération 7 : Train Score 0.595  ; Test Score 0.589\n",
      "Itération 8 : Train Score 0.601  ; Test Score 0.598\n",
      "Itération 9 : Train Score 0.598  ; Test Score 0.594\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "N_iter=10\n",
    "for i in range(N_iter):\n",
    "    # nous retrouvons ici le pattern typique MapReduce\n",
    "    # map: calcul des gradients pour chaque exemple\n",
    "    # reduce: aggrégation des gradients par somme\n",
    "    \n",
    "    #t0 = time()\n",
    "    grad_sum = train_main.map(lambda x: gradient(x, wb.value)).reduce(lambda x,y:x+y) \n",
    "    #print(grad_sum.take(2)[0].shape)\n",
    "    #t1 = time() - t0\n",
    "    #print(t1)\n",
    "    #t1 = time()\n",
    "    w = np.array(w)\n",
    "    #print(grad_sum.shape)\n",
    "    w = w - 0.5*grad_sum\n",
    "    #t2 = time() - t1\n",
    "    #print(t2)\n",
    "    #t2 = time()\n",
    "    #print(w.shape,grad_sum.shape)\n",
    "    wb = sc.broadcast(w)\n",
    "    train_nsc = train_main.map(lambda x: score(x, wb.value)).reduce(add)\n",
    "    #t3 = time() - t2\n",
    "    #print(t3)\n",
    "    test_nsc = test_main.map(lambda m: score(m, wb.value)).reduce(add)\n",
    "    print(\"Itération\", i, \": Train Score\", (train_nsc / train_main.count()).round(3) , \" ; Test Score\", (test_nsc / test_main.count()).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparaison des différentes approches - Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sans grande surprise, les fonctions implémentées au sein de la librairie MLLib sont plus rapides et donnent de meilleurs résultats que notre approche de gradient 'classique'. Cependant la régression logistique utilisée dans MLLIb est optimisée tandis que la notre ne l'est pas.\n",
    "Par ailleurs, la régression logistique sur-performe les approches SVM / NaiveBayes que nous avons également essayé via la librairie SparkMLLib.\n",
    "Les résultats sont en outre meilleurs lorsque l'on effectue un pré-traitement des données à l'aide de la méthode TF-IDF.\n",
    "\n",
    "Il est surtout très surprenant que dans les deux cas, sur le jeu de données complet, les algorithmes mettent beaucoup de temps à converger / itérer. Peut être que l'approche MapReduce est plus adaptée sur de plus gros jeux de données et sur des ordinateurs avec d'avantages de coeurs que sur nos MacBook Air.\n",
    "\n",
    "Ce travail préliminaire de découverte de PySpark gagnerait à être essayé sur de plus gros jeux de données et sur des ordinateurs plus puissants pour pouvoir constater le gain de temps comparé à une approche classique."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
